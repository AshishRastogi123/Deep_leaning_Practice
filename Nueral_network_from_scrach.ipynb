{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiX30sdyOc9Db4aaq51sa5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshishRastogi123/Deep_leaning_Practice/blob/main/Nueral_network_from_scrach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression from Scrach\n",
        "\n",
        "Logistic Regression used in Binary class classification which we used here\n",
        "Linear are used for multi class classification"
      ],
      "metadata": {
        "id": "sndE2gZ9iOiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "en9-vQyMiro7"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making training data\n",
        "# where we have imput --> (temp,rainfall, humidity) --> yield of apple and oranges crops\n",
        "inputs = np.array([\n",
        "    [73, 67, 43],\n",
        "    [91, 88, 64],\n",
        "    [87, 134, 58],\n",
        "    [182, 43, 37],\n",
        "    [69, 96, 78],], dtype='float32')"
      ],
      "metadata": {
        "id": "2fude3U_mXF_"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Target [apple and oranges]\n",
        "target =np.array([[56, 78],[81, 101],[119, 113], [22, 37],[103, 119]], dtype='float32')"
      ],
      "metadata": {
        "id": "oT91epwrn5ak"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert input and target in tensor\n",
        "inputs = torch.from_numpy(inputs)\n",
        "target=torch.from_numpy(target)\n",
        "\n",
        "print(inputs)\n",
        "print(target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTfqYuqGBiia",
        "outputId": "550c3663-e3be-4574-8ab4-198358d06423"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [182.,  43.,  37.],\n",
            "        [ 69.,  96.,  78.]])\n",
            "tensor([[ 56.,  78.],\n",
            "        [ 81., 101.],\n",
            "        [119., 113.],\n",
            "        [ 22.,  37.],\n",
            "        [103., 119.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Weights and biases\n",
        "w=torch.randn(2,3,requires_grad=True)\n",
        "b=torch.randn(2, requires_grad=True)\n",
        "\n",
        "print(w)\n",
        "print(b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y_XcC06CQhR",
        "outputId": "5ccf0026-f29d-4689-fcd5-e6548fc01294"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.4967,  0.5143, -1.5058],\n",
            "        [-2.0781, -0.8896, -0.2568]], requires_grad=True)\n",
            "tensor([ 0.0185, -0.0617], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model\n",
        "#apply linear regresiion\n",
        "def model(x):\n",
        "  return x @ w.t() +b #where @ denote matrix maltiplication and t() for transpose\n"
      ],
      "metadata": {
        "id": "U_L02KLVCxQ5"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w.t()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhSow8QvDVgv",
        "outputId": "8a77eb71-fc65-4fdb-d40f-58f1c6793499"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.4967, -2.0781],\n",
              "        [ 0.5143, -0.8896],\n",
              "        [-1.5058, -0.2568]], grad_fn=<TBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model(inputs)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SL7RuUEcDWo_",
        "outputId": "1cd30e0d-ff2a-48d7-dbd8-29807f29c0ef"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   5.9920, -222.4141],\n",
            "        [  -5.8868, -283.8956],\n",
            "        [  24.8210, -314.9662],\n",
            "        [  56.8232, -426.0402],\n",
            "        [ -33.7801, -248.8883]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#that not predict properly\n",
        "#So we will used loss function\n",
        "def MSE(y,y_hat):\n",
        "  diff=y -y_hat\n",
        "\n",
        "  return torch.sum(diff*diff)/diff.numel()"
      ],
      "metadata": {
        "id": "P-o8X6joDtV6"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Error or loss\n",
        "loss=MSE(target,pred)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxnESwx_nfky",
        "outputId": "0340f9a5-ef9a-4691-be93-fcf1f8415e90"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(81013.7500, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the Gradient\n",
        "loss.backward() # this function automatically gradient with respect to w and with respect to b which comes from y_hat comes from model function\n"
      ],
      "metadata": {
        "id": "PDu9zKJKoXtB"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)\n",
        "print(w.grad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXMcwqJYprUk",
        "outputId": "b4d48ec6-8ee8-4f37-89c0-33376f4199f6"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.4967,  0.5143, -1.5058],\n",
            "        [-2.0781, -0.8896, -0.2568]], requires_grad=True)\n",
            "tensor([[ -4570.1699,  -7050.0103,  -4510.7734],\n",
            "        [-40769.2812, -33314.8047, -21640.1875]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(b)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_KspLrop7hd",
        "outputId": "467b9097-4aee-40a8-8bb7-7f58018c5e20"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.0185, -0.0617], requires_grad=True)\n",
            "tensor([ -66.6061, -388.8409])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reset our gradient\n",
        "w.grad.zero_()\n",
        "b.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQLrdHf_qQwC",
        "outputId": "b1c44841-2925-4a98-d106-214b3dd4c4e3"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mx-QmJqzqfgB",
        "outputId": "9829a69f-bd74-45ff-ebca-aa0f3896a15b"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adjest parameter (prams)\n",
        "pred=model(inputs)\n",
        "loss=MSE(target,pred)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPcFAEZgqtG7",
        "outputId": "e2dcd3f9-2495-4621-e957-515af1d10799"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   5.9920, -222.4141],\n",
            "        [  -5.8868, -283.8956],\n",
            "        [  24.8210, -314.9662],\n",
            "        [  56.8232, -426.0402],\n",
            "        [ -33.7801, -248.8883]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR3mwbclq8wJ",
        "outputId": "a9c1765c-cf12-47a1-fa35-da7cc77ec31b"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ -4570.1699,  -7050.0103,  -4510.7734],\n",
            "        [-40769.2812, -33314.8047, -21640.1875]])\n",
            "tensor([ -66.6061, -388.8409])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate= 1e-5 # 1*10^-5\n",
        "\n",
        "with torch.no_grad():\n",
        "  # Perform in-place updates to w and b\n",
        "  w.sub_(w.grad * learning_rate)\n",
        "  b.sub_(b.grad * learning_rate)\n",
        "  # Now, reset the gradients to zero for the next iteration\n",
        "  w.grad.zero_()\n",
        "  b.grad.zero_()"
      ],
      "metadata": {
        "id": "NSLnYWRBrhRH"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsRQNgv4tSps",
        "outputId": "63022035-2245-4caa-de78-2b18be1effb6"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.5424,  0.5848, -1.4606],\n",
            "        [-1.6705, -0.5565, -0.0404]], requires_grad=True)\n",
            "tensor([ 0.0192, -0.0578], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate again\n",
        "preds= model(inputs)\n",
        "loss= MSE(target, preds)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUQ3kr16u2mf",
        "outputId": "0de21d12-a347-464f-c6be-43ffde418765"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(51192.7578, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training for multiple epoch\n",
        "for i in range(400):\n",
        "  preds= model(inputs)\n",
        "  loss = MSE(target, preds)\n",
        "  loss.backward()\n",
        "\n",
        "  # learning_rate= 1e-5 # 1*10^-5\n",
        "\n",
        "  with torch.no_grad():\n",
        "  # Perform in-place updates to w and b\n",
        "    w.sub_(w.grad * learning_rate)\n",
        "    b.sub_(b.grad * learning_rate)\n",
        "  # Now, reset the gradients to zero for the next iteration\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "  print(f\"Epoch: {i/100} and loss : {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRN2P6BUvJNk",
        "outputId": "f62fa5a5-544d-4a34-935b-980aaf8ea8ed"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0.0 and loss : 51192.7578125\n",
            "Epoch: 0.01 and loss : 32562.568359375\n",
            "Epoch: 0.02 and loss : 20915.22265625\n",
            "Epoch: 0.03 and loss : 13625.4169921875\n",
            "Epoch: 0.04 and loss : 9055.2236328125\n",
            "Epoch: 0.05 and loss : 6182.72119140625\n",
            "Epoch: 0.06 and loss : 4370.31640625\n",
            "Epoch: 0.07 and loss : 3220.17822265625\n",
            "Epoch: 0.08 and loss : 2484.0732421875\n",
            "Epoch: 0.09 and loss : 2007.0931396484375\n",
            "Epoch: 0.1 and loss : 1692.556640625\n",
            "Epoch: 0.11 and loss : 1480.1123046875\n",
            "Epoch: 0.12 and loss : 1332.074462890625\n",
            "Epoch: 0.13 and loss : 1224.900146484375\n",
            "Epoch: 0.14 and loss : 1143.8712158203125\n",
            "Epoch: 0.15 and loss : 1079.775146484375\n",
            "Epoch: 0.16 and loss : 1026.8389892578125\n",
            "Epoch: 0.17 and loss : 981.431640625\n",
            "Epoch: 0.18 and loss : 941.2654418945312\n",
            "Epoch: 0.19 and loss : 904.88671875\n",
            "Epoch: 0.2 and loss : 871.3663940429688\n",
            "Epoch: 0.21 and loss : 840.1028442382812\n",
            "Epoch: 0.22 and loss : 810.6993408203125\n",
            "Epoch: 0.23 and loss : 782.8885498046875\n",
            "Epoch: 0.24 and loss : 756.4845581054688\n",
            "Epoch: 0.25 and loss : 731.3521118164062\n",
            "Epoch: 0.26 and loss : 707.3902587890625\n",
            "Epoch: 0.27 and loss : 684.51904296875\n",
            "Epoch: 0.28 and loss : 662.6719970703125\n",
            "Epoch: 0.29 and loss : 641.7930908203125\n",
            "Epoch: 0.3 and loss : 621.8321533203125\n",
            "Epoch: 0.31 and loss : 602.7439575195312\n",
            "Epoch: 0.32 and loss : 584.4876708984375\n",
            "Epoch: 0.33 and loss : 567.02392578125\n",
            "Epoch: 0.34 and loss : 550.316650390625\n",
            "Epoch: 0.35 and loss : 534.3319091796875\n",
            "Epoch: 0.36 and loss : 519.0364990234375\n",
            "Epoch: 0.37 and loss : 504.3997497558594\n",
            "Epoch: 0.38 and loss : 490.39263916015625\n",
            "Epoch: 0.39 and loss : 476.986572265625\n",
            "Epoch: 0.4 and loss : 464.15484619140625\n",
            "Epoch: 0.41 and loss : 451.87213134765625\n",
            "Epoch: 0.42 and loss : 440.1136169433594\n",
            "Epoch: 0.43 and loss : 428.8563537597656\n",
            "Epoch: 0.44 and loss : 418.07769775390625\n",
            "Epoch: 0.45 and loss : 407.75677490234375\n",
            "Epoch: 0.46 and loss : 397.87298583984375\n",
            "Epoch: 0.47 and loss : 388.40673828125\n",
            "Epoch: 0.48 and loss : 379.3398742675781\n",
            "Epoch: 0.49 and loss : 370.6546936035156\n",
            "Epoch: 0.5 and loss : 362.3336486816406\n",
            "Epoch: 0.51 and loss : 354.36114501953125\n",
            "Epoch: 0.52 and loss : 346.72161865234375\n",
            "Epoch: 0.53 and loss : 339.39984130859375\n",
            "Epoch: 0.54 and loss : 332.3824157714844\n",
            "Epoch: 0.55 and loss : 325.6553649902344\n",
            "Epoch: 0.56 and loss : 319.20587158203125\n",
            "Epoch: 0.57 and loss : 313.02178955078125\n",
            "Epoch: 0.58 and loss : 307.0911560058594\n",
            "Epoch: 0.59 and loss : 301.40264892578125\n",
            "Epoch: 0.6 and loss : 295.9460144042969\n",
            "Epoch: 0.61 and loss : 290.71075439453125\n",
            "Epoch: 0.62 and loss : 285.686767578125\n",
            "Epoch: 0.63 and loss : 280.86492919921875\n",
            "Epoch: 0.64 and loss : 276.236328125\n",
            "Epoch: 0.65 and loss : 271.7923889160156\n",
            "Epoch: 0.66 and loss : 267.52496337890625\n",
            "Epoch: 0.67 and loss : 263.4261474609375\n",
            "Epoch: 0.68 and loss : 259.4885559082031\n",
            "Epoch: 0.69 and loss : 255.7050323486328\n",
            "Epoch: 0.7 and loss : 252.06881713867188\n",
            "Epoch: 0.71 and loss : 248.573486328125\n",
            "Epoch: 0.72 and loss : 245.2126922607422\n",
            "Epoch: 0.73 and loss : 241.9803466796875\n",
            "Epoch: 0.74 and loss : 238.87112426757812\n",
            "Epoch: 0.75 and loss : 235.8793487548828\n",
            "Epoch: 0.76 and loss : 233.0\n",
            "Epoch: 0.77 and loss : 230.22817993164062\n",
            "Epoch: 0.78 and loss : 227.5588836669922\n",
            "Epoch: 0.79 and loss : 224.9878692626953\n",
            "Epoch: 0.8 and loss : 222.5105438232422\n",
            "Epoch: 0.81 and loss : 220.12313842773438\n",
            "Epoch: 0.82 and loss : 217.8214874267578\n",
            "Epoch: 0.83 and loss : 215.6016082763672\n",
            "Epoch: 0.84 and loss : 213.46044921875\n",
            "Epoch: 0.85 and loss : 211.39413452148438\n",
            "Epoch: 0.86 and loss : 209.3997344970703\n",
            "Epoch: 0.87 and loss : 207.47390747070312\n",
            "Epoch: 0.88 and loss : 205.61349487304688\n",
            "Epoch: 0.89 and loss : 203.81582641601562\n",
            "Epoch: 0.9 and loss : 202.07815551757812\n",
            "Epoch: 0.91 and loss : 200.3978271484375\n",
            "Epoch: 0.92 and loss : 198.77243041992188\n",
            "Epoch: 0.93 and loss : 197.19949340820312\n",
            "Epoch: 0.94 and loss : 195.6767578125\n",
            "Epoch: 0.95 and loss : 194.2020721435547\n",
            "Epoch: 0.96 and loss : 192.77334594726562\n",
            "Epoch: 0.97 and loss : 191.38839721679688\n",
            "Epoch: 0.98 and loss : 190.04576110839844\n",
            "Epoch: 0.99 and loss : 188.7430877685547\n",
            "Epoch: 1.0 and loss : 187.47918701171875\n",
            "Epoch: 1.01 and loss : 186.25198364257812\n",
            "Epoch: 1.02 and loss : 185.05996704101562\n",
            "Epoch: 1.03 and loss : 183.90182495117188\n",
            "Epoch: 1.04 and loss : 182.77590942382812\n",
            "Epoch: 1.05 and loss : 181.6811065673828\n",
            "Epoch: 1.06 and loss : 180.6155242919922\n",
            "Epoch: 1.07 and loss : 179.57839965820312\n",
            "Epoch: 1.08 and loss : 178.5684051513672\n",
            "Epoch: 1.09 and loss : 177.58447265625\n",
            "Epoch: 1.1 and loss : 176.62530517578125\n",
            "Epoch: 1.11 and loss : 175.6900177001953\n",
            "Epoch: 1.12 and loss : 174.77755737304688\n",
            "Epoch: 1.13 and loss : 173.8868408203125\n",
            "Epoch: 1.14 and loss : 173.01727294921875\n",
            "Epoch: 1.15 and loss : 172.1675262451172\n",
            "Epoch: 1.16 and loss : 171.3371124267578\n",
            "Epoch: 1.17 and loss : 170.52505493164062\n",
            "Epoch: 1.18 and loss : 169.73065185546875\n",
            "Epoch: 1.19 and loss : 168.95330810546875\n",
            "Epoch: 1.2 and loss : 168.19198608398438\n",
            "Epoch: 1.21 and loss : 167.4462890625\n",
            "Epoch: 1.22 and loss : 166.7153778076172\n",
            "Epoch: 1.23 and loss : 165.99900817871094\n",
            "Epoch: 1.24 and loss : 165.29623413085938\n",
            "Epoch: 1.25 and loss : 164.60671997070312\n",
            "Epoch: 1.26 and loss : 163.92962646484375\n",
            "Epoch: 1.27 and loss : 163.26467895507812\n",
            "Epoch: 1.28 and loss : 162.61158752441406\n",
            "Epoch: 1.29 and loss : 161.96937561035156\n",
            "Epoch: 1.3 and loss : 161.33816528320312\n",
            "Epoch: 1.31 and loss : 160.7170867919922\n",
            "Epoch: 1.32 and loss : 160.10594177246094\n",
            "Epoch: 1.33 and loss : 159.50433349609375\n",
            "Epoch: 1.34 and loss : 158.91183471679688\n",
            "Epoch: 1.35 and loss : 158.32789611816406\n",
            "Epoch: 1.36 and loss : 157.75262451171875\n",
            "Epoch: 1.37 and loss : 157.1854248046875\n",
            "Epoch: 1.38 and loss : 156.62603759765625\n",
            "Epoch: 1.39 and loss : 156.07415771484375\n",
            "Epoch: 1.4 and loss : 155.52944946289062\n",
            "Epoch: 1.41 and loss : 154.9917449951172\n",
            "Epoch: 1.42 and loss : 154.46075439453125\n",
            "Epoch: 1.43 and loss : 153.93609619140625\n",
            "Epoch: 1.44 and loss : 153.41775512695312\n",
            "Epoch: 1.45 and loss : 152.90553283691406\n",
            "Epoch: 1.46 and loss : 152.3987274169922\n",
            "Epoch: 1.47 and loss : 151.89779663085938\n",
            "Epoch: 1.48 and loss : 151.40206909179688\n",
            "Epoch: 1.49 and loss : 150.91156005859375\n",
            "Epoch: 1.5 and loss : 150.42611694335938\n",
            "Epoch: 1.51 and loss : 149.9454345703125\n",
            "Epoch: 1.52 and loss : 149.46945190429688\n",
            "Epoch: 1.53 and loss : 148.99771118164062\n",
            "Epoch: 1.54 and loss : 148.53065490722656\n",
            "Epoch: 1.55 and loss : 148.06765747070312\n",
            "Epoch: 1.56 and loss : 147.60882568359375\n",
            "Epoch: 1.57 and loss : 147.15386962890625\n",
            "Epoch: 1.58 and loss : 146.70263671875\n",
            "Epoch: 1.59 and loss : 146.25521850585938\n",
            "Epoch: 1.6 and loss : 145.8112030029297\n",
            "Epoch: 1.61 and loss : 145.3709259033203\n",
            "Epoch: 1.62 and loss : 144.93374633789062\n",
            "Epoch: 1.63 and loss : 144.49989318847656\n",
            "Epoch: 1.64 and loss : 144.06930541992188\n",
            "Epoch: 1.65 and loss : 143.64163208007812\n",
            "Epoch: 1.66 and loss : 143.21701049804688\n",
            "Epoch: 1.67 and loss : 142.7952117919922\n",
            "Epoch: 1.68 and loss : 142.3762664794922\n",
            "Epoch: 1.69 and loss : 141.96011352539062\n",
            "Epoch: 1.7 and loss : 141.5465850830078\n",
            "Epoch: 1.71 and loss : 141.1354522705078\n",
            "Epoch: 1.72 and loss : 140.72715759277344\n",
            "Epoch: 1.73 and loss : 140.32107543945312\n",
            "Epoch: 1.74 and loss : 139.91751098632812\n",
            "Epoch: 1.75 and loss : 139.5162353515625\n",
            "Epoch: 1.76 and loss : 139.11727905273438\n",
            "Epoch: 1.77 and loss : 138.72042846679688\n",
            "Epoch: 1.78 and loss : 138.32589721679688\n",
            "Epoch: 1.79 and loss : 137.933349609375\n",
            "Epoch: 1.8 and loss : 137.54287719726562\n",
            "Epoch: 1.81 and loss : 137.1544647216797\n",
            "Epoch: 1.82 and loss : 136.76815795898438\n",
            "Epoch: 1.83 and loss : 136.38357543945312\n",
            "Epoch: 1.84 and loss : 136.00094604492188\n",
            "Epoch: 1.85 and loss : 135.6200714111328\n",
            "Epoch: 1.86 and loss : 135.2410888671875\n",
            "Epoch: 1.87 and loss : 134.86395263671875\n",
            "Epoch: 1.88 and loss : 134.4883575439453\n",
            "Epoch: 1.89 and loss : 134.11483764648438\n",
            "Epoch: 1.9 and loss : 133.74273681640625\n",
            "Epoch: 1.91 and loss : 133.37232971191406\n",
            "Epoch: 1.92 and loss : 133.0035400390625\n",
            "Epoch: 1.93 and loss : 132.63650512695312\n",
            "Epoch: 1.94 and loss : 132.27076721191406\n",
            "Epoch: 1.95 and loss : 131.90670776367188\n",
            "Epoch: 1.96 and loss : 131.5442352294922\n",
            "Epoch: 1.97 and loss : 131.18313598632812\n",
            "Epoch: 1.98 and loss : 130.82345581054688\n",
            "Epoch: 1.99 and loss : 130.4654083251953\n",
            "Epoch: 2.0 and loss : 130.10867309570312\n",
            "Epoch: 2.01 and loss : 129.75341796875\n",
            "Epoch: 2.02 and loss : 129.3994140625\n",
            "Epoch: 2.03 and loss : 129.04684448242188\n",
            "Epoch: 2.04 and loss : 128.6956329345703\n",
            "Epoch: 2.05 and loss : 128.34576416015625\n",
            "Epoch: 2.06 and loss : 127.99715423583984\n",
            "Epoch: 2.07 and loss : 127.6498794555664\n",
            "Epoch: 2.08 and loss : 127.3038558959961\n",
            "Epoch: 2.09 and loss : 126.95915222167969\n",
            "Epoch: 2.1 and loss : 126.61556243896484\n",
            "Epoch: 2.11 and loss : 126.2734146118164\n",
            "Epoch: 2.12 and loss : 125.93221282958984\n",
            "Epoch: 2.13 and loss : 125.59251403808594\n",
            "Epoch: 2.14 and loss : 125.25386810302734\n",
            "Epoch: 2.15 and loss : 124.91642761230469\n",
            "Epoch: 2.16 and loss : 124.58018493652344\n",
            "Epoch: 2.17 and loss : 124.24505615234375\n",
            "Epoch: 2.18 and loss : 123.9111557006836\n",
            "Epoch: 2.19 and loss : 123.57833099365234\n",
            "Epoch: 2.2 and loss : 123.24686431884766\n",
            "Epoch: 2.21 and loss : 122.91615295410156\n",
            "Epoch: 2.22 and loss : 122.58687591552734\n",
            "Epoch: 2.23 and loss : 122.25843811035156\n",
            "Epoch: 2.24 and loss : 121.93128967285156\n",
            "Epoch: 2.25 and loss : 121.6050796508789\n",
            "Epoch: 2.26 and loss : 121.280029296875\n",
            "Epoch: 2.27 and loss : 120.95601654052734\n",
            "Epoch: 2.28 and loss : 120.633056640625\n",
            "Epoch: 2.29 and loss : 120.31131744384766\n",
            "Epoch: 2.3 and loss : 119.9903793334961\n",
            "Epoch: 2.31 and loss : 119.670654296875\n",
            "Epoch: 2.32 and loss : 119.35186767578125\n",
            "Epoch: 2.33 and loss : 119.03416442871094\n",
            "Epoch: 2.34 and loss : 118.71745300292969\n",
            "Epoch: 2.35 and loss : 118.40181732177734\n",
            "Epoch: 2.36 and loss : 118.08712005615234\n",
            "Epoch: 2.37 and loss : 117.7734146118164\n",
            "Epoch: 2.38 and loss : 117.4607925415039\n",
            "Epoch: 2.39 and loss : 117.14888763427734\n",
            "Epoch: 2.4 and loss : 116.83824157714844\n",
            "Epoch: 2.41 and loss : 116.52854919433594\n",
            "Epoch: 2.42 and loss : 116.21968841552734\n",
            "Epoch: 2.43 and loss : 115.9120101928711\n",
            "Epoch: 2.44 and loss : 115.60520935058594\n",
            "Epoch: 2.45 and loss : 115.2992172241211\n",
            "Epoch: 2.46 and loss : 114.99430847167969\n",
            "Epoch: 2.47 and loss : 114.6902847290039\n",
            "Epoch: 2.48 and loss : 114.38724517822266\n",
            "Epoch: 2.49 and loss : 114.08509826660156\n",
            "Epoch: 2.5 and loss : 113.783935546875\n",
            "Epoch: 2.51 and loss : 113.48371887207031\n",
            "Epoch: 2.52 and loss : 113.1844253540039\n",
            "Epoch: 2.53 and loss : 112.8859634399414\n",
            "Epoch: 2.54 and loss : 112.58856201171875\n",
            "Epoch: 2.55 and loss : 112.2918930053711\n",
            "Epoch: 2.56 and loss : 111.9963150024414\n",
            "Epoch: 2.57 and loss : 111.70152282714844\n",
            "Epoch: 2.58 and loss : 111.4077377319336\n",
            "Epoch: 2.59 and loss : 111.11474609375\n",
            "Epoch: 2.6 and loss : 110.82254791259766\n",
            "Epoch: 2.61 and loss : 110.53144836425781\n",
            "Epoch: 2.62 and loss : 110.24112701416016\n",
            "Epoch: 2.63 and loss : 109.95174407958984\n",
            "Epoch: 2.64 and loss : 109.66325378417969\n",
            "Epoch: 2.65 and loss : 109.37550354003906\n",
            "Epoch: 2.66 and loss : 109.0887222290039\n",
            "Epoch: 2.67 and loss : 108.80278015136719\n",
            "Epoch: 2.68 and loss : 108.517822265625\n",
            "Epoch: 2.69 and loss : 108.23359680175781\n",
            "Epoch: 2.7 and loss : 107.95027923583984\n",
            "Epoch: 2.71 and loss : 107.667724609375\n",
            "Epoch: 2.72 and loss : 107.38606262207031\n",
            "Epoch: 2.73 and loss : 107.10527038574219\n",
            "Epoch: 2.74 and loss : 106.8253173828125\n",
            "Epoch: 2.75 and loss : 106.54620361328125\n",
            "Epoch: 2.76 and loss : 106.2680435180664\n",
            "Epoch: 2.77 and loss : 105.99055480957031\n",
            "Epoch: 2.78 and loss : 105.71405029296875\n",
            "Epoch: 2.79 and loss : 105.43824768066406\n",
            "Epoch: 2.8 and loss : 105.16341400146484\n",
            "Epoch: 2.81 and loss : 104.88923645019531\n",
            "Epoch: 2.82 and loss : 104.61592102050781\n",
            "Epoch: 2.83 and loss : 104.34342193603516\n",
            "Epoch: 2.84 and loss : 104.07179260253906\n",
            "Epoch: 2.85 and loss : 103.801025390625\n",
            "Epoch: 2.86 and loss : 103.5309829711914\n",
            "Epoch: 2.87 and loss : 103.2617416381836\n",
            "Epoch: 2.88 and loss : 102.99337005615234\n",
            "Epoch: 2.89 and loss : 102.7257308959961\n",
            "Epoch: 2.9 and loss : 102.45896911621094\n",
            "Epoch: 2.91 and loss : 102.19293212890625\n",
            "Epoch: 2.92 and loss : 101.92762756347656\n",
            "Epoch: 2.93 and loss : 101.6633071899414\n",
            "Epoch: 2.94 and loss : 101.39967346191406\n",
            "Epoch: 2.95 and loss : 101.13675689697266\n",
            "Epoch: 2.96 and loss : 100.87467193603516\n",
            "Epoch: 2.97 and loss : 100.6134033203125\n",
            "Epoch: 2.98 and loss : 100.35295104980469\n",
            "Epoch: 2.99 and loss : 100.0932388305664\n",
            "Epoch: 3.0 and loss : 99.834228515625\n",
            "Epoch: 3.01 and loss : 99.57609558105469\n",
            "Epoch: 3.02 and loss : 99.31864929199219\n",
            "Epoch: 3.03 and loss : 99.06199645996094\n",
            "Epoch: 3.04 and loss : 98.80615234375\n",
            "Epoch: 3.05 and loss : 98.55091857910156\n",
            "Epoch: 3.06 and loss : 98.29671478271484\n",
            "Epoch: 3.07 and loss : 98.04298400878906\n",
            "Epoch: 3.08 and loss : 97.79023742675781\n",
            "Epoch: 3.09 and loss : 97.53814697265625\n",
            "Epoch: 3.1 and loss : 97.28683471679688\n",
            "Epoch: 3.11 and loss : 97.03611755371094\n",
            "Epoch: 3.12 and loss : 96.786376953125\n",
            "Epoch: 3.13 and loss : 96.53720092773438\n",
            "Epoch: 3.14 and loss : 96.288818359375\n",
            "Epoch: 3.15 and loss : 96.04118347167969\n",
            "Epoch: 3.16 and loss : 95.79425811767578\n",
            "Epoch: 3.17 and loss : 95.54820251464844\n",
            "Epoch: 3.18 and loss : 95.30268859863281\n",
            "Epoch: 3.19 and loss : 95.05801391601562\n",
            "Epoch: 3.2 and loss : 94.81412506103516\n",
            "Epoch: 3.21 and loss : 94.57084655761719\n",
            "Epoch: 3.22 and loss : 94.32835388183594\n",
            "Epoch: 3.23 and loss : 94.0865478515625\n",
            "Epoch: 3.24 and loss : 93.84544372558594\n",
            "Epoch: 3.25 and loss : 93.6050796508789\n",
            "Epoch: 3.26 and loss : 93.36537170410156\n",
            "Epoch: 3.27 and loss : 93.12644958496094\n",
            "Epoch: 3.28 and loss : 92.88832092285156\n",
            "Epoch: 3.29 and loss : 92.65079498291016\n",
            "Epoch: 3.3 and loss : 92.4139175415039\n",
            "Epoch: 3.31 and loss : 92.17793273925781\n",
            "Epoch: 3.32 and loss : 91.94242858886719\n",
            "Epoch: 3.33 and loss : 91.70765686035156\n",
            "Epoch: 3.34 and loss : 91.4736557006836\n",
            "Epoch: 3.35 and loss : 91.24034881591797\n",
            "Epoch: 3.36 and loss : 91.00770568847656\n",
            "Epoch: 3.37 and loss : 90.77581787109375\n",
            "Epoch: 3.38 and loss : 90.54457092285156\n",
            "Epoch: 3.39 and loss : 90.31401824951172\n",
            "Epoch: 3.4 and loss : 90.0842056274414\n",
            "Epoch: 3.41 and loss : 89.85491180419922\n",
            "Epoch: 3.42 and loss : 89.62648010253906\n",
            "Epoch: 3.43 and loss : 89.39860534667969\n",
            "Epoch: 3.44 and loss : 89.17141723632812\n",
            "Epoch: 3.45 and loss : 88.94497680664062\n",
            "Epoch: 3.46 and loss : 88.7192153930664\n",
            "Epoch: 3.47 and loss : 88.49396514892578\n",
            "Epoch: 3.48 and loss : 88.26957702636719\n",
            "Epoch: 3.49 and loss : 88.04579162597656\n",
            "Epoch: 3.5 and loss : 87.82252502441406\n",
            "Epoch: 3.51 and loss : 87.60014343261719\n",
            "Epoch: 3.52 and loss : 87.37825775146484\n",
            "Epoch: 3.53 and loss : 87.15715026855469\n",
            "Epoch: 3.54 and loss : 86.93663024902344\n",
            "Epoch: 3.55 and loss : 86.71678161621094\n",
            "Epoch: 3.56 and loss : 86.49755096435547\n",
            "Epoch: 3.57 and loss : 86.27906799316406\n",
            "Epoch: 3.58 and loss : 86.06111907958984\n",
            "Epoch: 3.59 and loss : 85.8438949584961\n",
            "Epoch: 3.6 and loss : 85.62735748291016\n",
            "Epoch: 3.61 and loss : 85.41130065917969\n",
            "Epoch: 3.62 and loss : 85.19600677490234\n",
            "Epoch: 3.63 and loss : 84.98135375976562\n",
            "Epoch: 3.64 and loss : 84.76722717285156\n",
            "Epoch: 3.65 and loss : 84.55384826660156\n",
            "Epoch: 3.66 and loss : 84.34114074707031\n",
            "Epoch: 3.67 and loss : 84.12892150878906\n",
            "Epoch: 3.68 and loss : 83.91740417480469\n",
            "Epoch: 3.69 and loss : 83.70654296875\n",
            "Epoch: 3.7 and loss : 83.49622344970703\n",
            "Epoch: 3.71 and loss : 83.28657531738281\n",
            "Epoch: 3.72 and loss : 83.07769012451172\n",
            "Epoch: 3.73 and loss : 82.86921691894531\n",
            "Epoch: 3.74 and loss : 82.66142272949219\n",
            "Epoch: 3.75 and loss : 82.45433044433594\n",
            "Epoch: 3.76 and loss : 82.24772644042969\n",
            "Epoch: 3.77 and loss : 82.0418472290039\n",
            "Epoch: 3.78 and loss : 81.83646392822266\n",
            "Epoch: 3.79 and loss : 81.6316909790039\n",
            "Epoch: 3.8 and loss : 81.4276351928711\n",
            "Epoch: 3.81 and loss : 81.22407531738281\n",
            "Epoch: 3.82 and loss : 81.02127075195312\n",
            "Epoch: 3.83 and loss : 80.81895446777344\n",
            "Epoch: 3.84 and loss : 80.61715698242188\n",
            "Epoch: 3.85 and loss : 80.41613006591797\n",
            "Epoch: 3.86 and loss : 80.21566009521484\n",
            "Epoch: 3.87 and loss : 80.01570129394531\n",
            "Epoch: 3.88 and loss : 79.81645202636719\n",
            "Epoch: 3.89 and loss : 79.61778259277344\n",
            "Epoch: 3.9 and loss : 79.4195556640625\n",
            "Epoch: 3.91 and loss : 79.22206115722656\n",
            "Epoch: 3.92 and loss : 79.02510833740234\n",
            "Epoch: 3.93 and loss : 78.82872009277344\n",
            "Epoch: 3.94 and loss : 78.63282775878906\n",
            "Epoch: 3.95 and loss : 78.43769836425781\n",
            "Epoch: 3.96 and loss : 78.24309539794922\n",
            "Epoch: 3.97 and loss : 78.04895782470703\n",
            "Epoch: 3.98 and loss : 77.8556137084961\n",
            "Epoch: 3.99 and loss : 77.66266632080078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here we start loss at  51192.7578125 to reduce it till the 77.66266632080078\n",
        "pred=model(inputs)\n",
        "loss = MSE(target , preds)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg9oiEU8wLjj",
        "outputId": "9a39af78-3dfa-44d1-85ab-b1de3e7dd014"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(77.6627, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "sqrt(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6owE7jlhw4JD",
        "outputId": "1a6464c5-329b-478d-dbc3-8f8ad7669ac6"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3167310485.py:2: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)\n",
            "  sqrt(loss)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.812642414213842"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6OJRl4DxDYX",
        "outputId": "08d325fc-7e97-4767-be1d-4fb6a0d483f6"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 60.1253,  69.4059],\n",
              "        [ 76.9834,  98.5307],\n",
              "        [135.3792, 119.5272],\n",
              "        [ 21.8672,  38.9671],\n",
              "        [ 84.3467, 118.1035]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0jzFWVqxIqg",
        "outputId": "ae957b46-e227-425b-badc-db3f677ac18d"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 56.,  78.],\n",
              "        [ 81., 101.],\n",
              "        [119., 113.],\n",
              "        [ 22.,  37.],\n",
              "        [103., 119.]])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we very closed to prediction"
      ],
      "metadata": {
        "id": "oANaTqG1xOZq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}